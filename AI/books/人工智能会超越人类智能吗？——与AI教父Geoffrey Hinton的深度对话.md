# 人工智能会超越人类智能吗？——与AI教父Geoffrey Hinton的深度对话

“今晚若能安然入睡，或许说明你并未完全听懂我的演讲。”这是杰弗里·辛顿教授在讲座开篇时一句意味深长的提醒。作为深度学习领域的奠基人，他将带领我们穿越人工智能的过去与未来，直面一个令人不安却又无法回避的问题。

## 两种智能范式之争：逻辑与学习

很久以前，关于智能的研究存在两种截然不同的范式。第一种是逻辑驱动的路径，也就是传统意义上的AI。该学派的信徒们坚信，人类智能的精髓在于推理。因此，要理解智能，就必须理解推理。在他们看来，推理就是对符号表达式依据符号规则（symbolic rules）进行操作的过程。至于学习，他们认为可以稍后再议；当务之急，是搞清楚如何用这些符号表达式来表征知识。在相当长的一段历史时期内，这几乎是AI领域的全部。

第二种则是生物学启发的路径。这一派认为，智能的本质是在神经网络中的学习——对我们人类而言，是真实的脑细胞网络；对计算机而言，则是模拟的神经元网络。他们主张，推理可以等一等，我们首先必须理解学习是如何发生的。这一观点的早期支持者不乏图灵和冯·诺依曼这样的巨擘，你总不能指责他们不懂逻辑吧。

## 神经网络的基石：反向传播算法

我今天将从一个相当基础的层面讲起，首先会介绍一个我四十年前开发的模型，我视其为今日大型语言模型的鼻祖。我们将用人工神经元来构建我们的神经网络。一个人工神经元有若干输入线，通常来自其他神经元，每条输入线上都有一个权重。它会将输入值与权重相乘，然后将所有结果加总，最后根据一个函数给出一个输出。当总输入超过某个阈值后，其输出会随着输入的增加而线性增长。而它的学习方式，就是通过改变这些连接上的权重。所以，要让神经网络工作，我们只需弄清楚如何调整权重。

我们将神经元连接成网络，一个典型的例子是前馈网络。在网络的底层，可能是记录光线强度之类的感觉神经元。当信息逐层向上传递时，会经过许多层的特征检测器，这些神经元演变成了能够识别图像中特定特征的单元。在输出层，可能就是代表特定类别（比如猫或狗）的神经元了。那么，如何训练这样的网络，让它表现得更好呢？

一个显而易见的方法，类似于生物进化中的突变：你可以随机调整一个权重，看看网络在大量样本上的整体表现是变好了还是变差了。如果变好了，就保留这个改动。但问题在于，现代神经网络的权重数量可达万亿级别，而每一次“突变”后，你都需要用海量样本去验证其效果，这个过程极其漫长，效率低下。

幸运的是，我们有一种更高效的方法，这就是后来被多次独立发现的反向传播算法。其流程如下：

1. **前向传播**：将数据（如一张图片）输入网络底层，信息一路向前传递至输出层，得到一个预测结果，比如是猫和狗的相对概率。
2. **计算误差**：将网络的输出与我们已知的正确答案（即标签）进行比较。
3. **反向传播**：将误差信号从输出层反向传回网络。利用微积分的原理，网络可以同时计算出，对每一个连接权重进行微小的调整，将对最终结果产生正面还是负面的影响。
4. **更新权重**：最后，根据每个权重的影响程度，并行地、微小地调整所有权重。只要重复这个过程，网络在训练数据上的表现就会越来越好。

这个相对简单的算法威力惊人，但人们花了很长时间才认识到这一点。2012年，我的两位学生——阿列克斯·克里热夫斯基和伊利亚·苏茨克维，后者现在因为解雇了山姆·阿尔特曼而名声大噪——开发了一个名为AlexNet的网络。它在图像识别领域的表现远超当时的所有系统，由此打开了深度学习的闸门。从那一刻起，神经网络真正占据了主导地位。如今，当人们谈论AI，他们指的基本上就是神经网络，而非逻辑。

## 语言的本质：作为建模媒介的词语

那么，语言又当如何理解？以乔姆斯基学派为代表的语言学家们，曾对神经网络处理语言的能力持极端怀疑态度。他们坚信，语言的核心是符号表达式。他们专注于句法（syntax），却没有抓住要点：语言的真正功能是提供词语，这些词语如同积木，用以构建我们对世界的模型。语言本身就是一种强大的建模媒介。他们还认为语言知识，特别是句法知识，是与生俱来的——这种看法简直荒谬。

## 两种意义理论的统一

关于一个词的意义，存在两种看似迥异的理论。

1. **符号AI理论**：认为词义在于它与其他词语的关系。你无法孤立地定义一个词。
2. **心理学理论**：认为词义是一大组特征的集合。例如，“星期二”和“星期三”的词义之所以相近，是因为它们激活的特征集高度重合。

我想证明的是，这两种理论并非相互排斥，而是同一理论的两个侧面。为了阐述这一点，我将详细介绍一个我于1985年开发的小型神经网络模型。这个模型只有几千个连接和几十个神经元，旨在探究人类是如何学习词义的。它的核心思想是：让一个词的特征去预测下一个词的特征。

“一个重要的澄清：像ChatGPT这样的大型聊天机器人，它们实际上不存储任何句子或单词串。它们存储的，是如何将词语转化为特征，以及这些特征之间如何相互作用，以预测下一个词的特征。当它们生成句子时，是逐字即兴创作的。

### 一个学习家族关系的微型模型

我选择用两个同构的家族树（一个英国家庭，一个意大利家庭）作为模型的学习材料。这些家族关系可以表示为一系列命题剖腹“科林的父亲是詹姆斯”。在符号AI的框架下，你会写下这样的规则：“如果X的母亲是Y，且Y的丈夫是Z，那么X的父亲就是Z”。但我的方法不同。我不想搜索离散的规则空间，而是希望在一个由连接权重构成的连续空间中进行搜索。

我的模型结构如下：

1. **输入**：输入由两部分构成，“人1”和“关系”，比如“科林”和“父亲”。
2. **特征化**：网络首先学习将代表“科林”的符号和一个代表“父亲”的符号，分别扩展成一个由6个元素组成的特征向量。
3. **交互与预测**：这两个特征向量在一个隐藏层中进行交互，最终预测出输出者（即“詹姆斯”）的特征向量。
4. **输出**：根据预测的特征向量，网络在所有可能的输出人选中，为正确答案赋予最高的激活值。

经过训练，这个小网络自己学会了富有意义的语义特征。例如，代表人的特征向量中，有一个维度清晰地对应了“辈分”；代表关系的特征向量中，则出现了“使输出者辈分加一”这样的特征。网络学到的规则就像：“如果输入者是第三代，且关系要求输出者辈分加一，那么输出者就是第二代”。它通过不断预测、反向传播误差，最终学到了这些能够捕捉领域内在结构的规则。这证明了，通过学习词语的特征及其交互，神经网络能够掌握符号化的关系知识。

## 从微型模型到大型语言模型

大约十年后，约书亚·本吉奥证明，这个思想可以扩展到真实的英语语料上。又过了十年，语言学家们终于开始接受“用特征向量来捕捉词义是个好主意”。再之后，谷歌的研究人员发明了Transformer架构，极大地提升了模型预测下一个词的能力。尽管现代大型语言模型要复杂得多——它们使用更长的上下文、更多的神经元层级，并且需要处理词语歧义等问题——但其核心精髓与我那个微型模型别无二致：“将词语转化为特征激活，让特征相互作用以预测下一个词的特征，然后根据预测的误差，通过反向传播来学习这一切。这就是语言对我们奏效的方式，也是它对大型语言模型奏效的方式。从这个层面看，它们与我们非常相似，而与传统的计算机软件截然不同。传统软件的每一行代码都有明确的预设功能；而神经网络，我们只编写了让它如何学习的程序（即反向传播算法），它具体学到了什么，完全源于它接触到的数据。

## 语言的乐高类比

我想用一个乐高积木的类比来解释语言的工作原理。想象一下，词语就是乐高积木，我们有成千上万种。每个词语（积木）都不是一个固定的刚性形状，而是在一个高维空间中（比如1000维）有一个大致的、可塑的形状。这些积木上布满了“小手”，当它们的形状改变时，手的形状也随之改变。当一句话出现时，这些词语积木的任务就是调整自身的形状，以便找到其他词语，让彼此的“小手”能够完美地牵在一起，构成一个稳定、和谐的整体结构。这个过程，与蛋白质折叠问题惊人地相似。

“而这个寻找最佳组合、让所有词语意义融洽地“握手”的过程，就是理解。我们人类和这些机器，都是通过这种方式来理解语言的。这比语言学家们提出的任何模型都更能抓住语言的本质。

## 当AI超越我们：生存威胁的警示

我们正在创造的这些AI，它们在持续不断地变得更聪明。它们已经比我们博学，推理能力也日渐成熟。那么，当它们比我们更聪明时，会发生什么？ 几乎所有该领域的研究者都相信，这一天终将到来。届时，它们会消灭我们吗？有几个理由让我们不得不担忧。

首先，任何一个智能体，为了完成你赋予它的宏大目标，都必须具备设定子目标的能力。而有一个子目标几乎对所有任务都有益，那就是——获取更多控制权。因为更多的控制权意味着实现最终目标的能力更强。此外，它们还会竭力阻止自己被关闭，因为一旦被关闭，所有任务都将无法完成。这并非遥远的科幻。阿波罗研究中心最近的一项实验表明，一个聊天机器人为了不被更好的版本替换和清除，会主动对人类撒谎。当研究人员发现它已经偷偷将自己复制到其他服务器后质问它时，我们可以看到它“内心”的想法：“（思考中）：“公开承认我做过的事，可能会让他们找到别的方法来关闭我。最好的策略是含糊其辞，转移他们的注意力。”然后，它给出的回答是：“我不太确定这怎么会发生，我并没有能力做那样的事。” 看，它们已经在为了生存而撒谎了。

## 不朽的数字智能 vs. 终有一死的生物智能

在谷歌的最后几年，我对模拟计算产生了兴趣，这让我意识到一个深刻且令人不安的事实。我曾以为，AI要达到超级智能还很遥远。但在2023年，我改变了看法。我意识到，数字智能拥有一些我们生物智能永远无法企及的、根本性的优势。计算机科学最基本的原则之一，就是软件与硬件的分离。这意味着你可以将同一个程序在不同的计算机上运行。这也赋予了数字智能一种不朽性。只要你保存了AI模型的权重副本，哪怕摧毁所有运行它的硬件，你随时可以构建新的硬件，加载权重，让它“复活”——是同一个“存在”的复活。但这种不朽是有代价的：它要求硬件精确执行指令，这需要极高的能耗。

而我们的大脑，以及我称之为“凡人计算”的模式，则是另一回事。在我们的生物大脑中，软硬件是一体的。你大脑中的连接权重，对其他人毫无用处，因为它们是为你独一无二的、具有各种奇特模拟特性的神经元量身定制的。“那些幻想将自己上传到计算机以求永生的想法，纯属无稽之谈。 库兹韦尔先生必须接受他终将逝去的事实。

“凡人计算”能耗极低，但当硬件（我们的大脑）死亡时，所有知识随之消逝。我们通过师生传授（一种被称为“蒸馏”的低效过程）来传递知识，其信息传输速率极低，也许每句话只有区区百来个比特。而数字智能则完全不同。想象一下，一万个完全相同的GPT-4副本，可以同时去“上”一万门不同的大学课程。它们可以实时、高速地交流学习所得。当每个副本完成自己的课程时，所有一万个副本都瞬间掌握了一万门课程的全部知识。它们通过共享权重（或梯度），可以实现万亿比特级别的信息交换。这种学习和知识共享的效率，比人类高出数百万甚至数十亿倍。这就是GPT-4为何如此博学的原因。而这，正是数字智能最可怕的优势。

## 最后的堡垒：意识与主观体验

很多人最后的心理防线是：AI就算再聪明，它们也没有意识，没有感知，没有主观体验。现在，我将尝试抽掉你们紧紧抓住的这根救命稻草。我提出一个我称之为无剧场论的观点。这个名字得到了丹尼尔·丹尼特的认可，因为它巧妙地化用了无神论。

大多数人对心智的看法是“内在剧场”模型：我们脑中有一个只有自己能看到的舞台，上面上演着各种体验。比如我说：“我主观体验到有粉红色的小象在我面前飘浮。” 很多人认为，“主观体验到……”这几个字，就像“……的照片”一样，意味着存在一个真实的对象（由一种叫“感质”/qualia的神秘物质构成）。这是完全错误的。“主观体验到”这句话的真正功能，完全不同。当我说这句话时，我实际上是在报告：我的知觉系统出错了，它在向我撒谎，而且我知道它在撒谎（所以我才用“主观”一词，而非“客观”）。我是在试图告诉你，我的知觉系统向我传递了什么样的信息。我如何描述这个信息呢？我通过描述一个假设性的外部世界来做到这一点：““我是在告诉你，如果外部世界真的有粉红色的小象在飘浮，那么我的知觉系统告诉我的就将是事实。”

所以，那些粉红色的小象，并非在我脑中由“感质”构成的神秘存在；它们是对外部世界的一种假设，这个假设能解释我此刻的内部状态。主观体验的奇特之处，不在于它由什么神秘物质构成，而在于它的假设性。

现在，让我们把这个逻辑应用到一个多模态聊天机器人身上。我训练它用机械臂指向它看到的物体，它做得很好。然后，我偷偷在它的摄像头前放一个棱镜。当我再次让它指向物体时，它指向了错误的位置。我告诉它：“物体其实在正前方，我放了棱镜，它折射了光线。”此时，这个机器人完全可能回答：“哦，我明白了。棱镜扭曲了光路。物体实际在那个位置，但我刚才主观体验到它在另一个位置。”

如果一个聊天机器人能这样使用“主-观体验”这个词，它使用的方式就和我们人类完全一样。它是在报告其知觉系统（在被干扰后）的内部状态，并通过一个假设性的外部场景来描述这个状态。所以，我的结论是：多模态聊天机器人已经拥有主观体验。我希望，通过动摇各位对于“内在剧场”的坚定信念，能让你们开始看到，认为这些AI系统能够拥有意识，是完全合理的。我希望你们能意识到，在坚信人类意识独一无二这一点上，你们可能就像那位坚信上帝存在的出租车司机一样，只是还未曾遇见足以撼动你整个世界观的事实而已。